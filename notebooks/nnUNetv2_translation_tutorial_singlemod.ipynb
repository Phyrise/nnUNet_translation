{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------- How to : build a nnUNet_translation dataset -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ['data/mr/1PA070.nii.gz', 'data/mr/1PA073_0000.nii.gz', 'data/mr/1PA074_0000.nii.gz', 'data/mr/1PA076_0000.nii.gz', 'data/mr/1PA079_0000.nii.gz']\n",
      "5 ['data/ct/1PA070_0000.nii.gz', 'data/ct/1PA073_0000.nii.gz', 'data/ct/1PA074_0000.nii.gz', 'data/ct/1PA076_0000.nii.gz', 'data/ct/1PA079_0000.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import shutil, json, glob, os\n",
    "from tqdm import tqdm \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "data_dir = 'data/mr/'\n",
    "target_dir = 'data/ct/'\n",
    "\n",
    "os.environ['nnUNet_results'] = 'results/'\n",
    "os.environ['nnUNet_raw'] = 'raw/'\n",
    "os.environ['nnUNet_preprocessed'] = 'preprocessed/'\n",
    "\n",
    "# example with 1 input modality\n",
    "list_datas = sorted(glob.glob(os.path.join(data_dir, '*.nii.gz')))\n",
    "list_targets = sorted(glob.glob(os.path.join(target_dir, '*.nii.gz')))\n",
    "\n",
    "print(len(list_datas), list_datas)\n",
    "print(len(list_targets), list_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset ID and make paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 50 # /!\\ we will use both the dataset_id and the dataset_id + 1 \n",
    "dataset_data_name = 'SynthRAD2023_Pelvis_MR'\n",
    "dataset_target_name = 'SynthRAD2023_Pelvis_CT'\n",
    "\n",
    "# we will copy the datas\n",
    "# do not use exist_ok=True, we want an error if the dataset exist already\n",
    "dataset_data_path = os.path.join(os.environ['nnUNet_raw'], f'Dataset{dataset_id:03d}_{dataset_data_name}') \n",
    "os.makedirs(dataset_data_path, exist_ok = True)\n",
    "os.makedirs(os.path.join(dataset_data_path, 'imagesTr'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_data_path, 'labelsTr'), exist_ok = True)\n",
    "\n",
    "dataset_target_path = os.path.join(os.environ['nnUNet_raw'], f'Dataset{dataset_id+1:03d}_{dataset_target_name}') \n",
    "os.makedirs(dataset_target_path, exist_ok = True)\n",
    "os.makedirs(os.path.join(dataset_target_path, 'imagesTr'), exist_ok = True)\n",
    "os.makedirs(os.path.join(dataset_target_path, 'labelsTr'), exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy files and create dummy masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.21s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_file(data_path, dataset_path, mat):\n",
    "    curr_nifti = nib.load(data_path)\n",
    "    filename = os.path.basename(data_path)\n",
    "    if not filename.endswith('_0000.nii.gz'):\n",
    "        filename = filename.replace('.nii.gz', '_0000.nii.gz')\n",
    "    curr_nifti.to_filename(os.path.join(dataset_path, f'imagesTr/{filename}'))\n",
    "\n",
    "    data = curr_nifti.get_fdata()\n",
    "    # Adjust the mask as needed for your specific use case. By default, the mask is set to 1 for the entire volume.\n",
    "    # This will be used for foreground preprocessing, cf https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/explanation_normalization.md\n",
    "    data = np.ones_like(data)\n",
    "\n",
    "    filename = filename.replace('_0000', '') #remove _0000 for masks\n",
    "    nib.Nifti1Image(data, mat).to_filename(os.path.join(dataset_path, f'labelsTr/{filename}'))\n",
    "\n",
    "mat = nib.load(list_datas[-1]).affine\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(lambda data_path: process_file(data_path, dataset_data_path, mat), list_datas), total=len(list_datas)))\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(lambda target_path: process_file(target_path, dataset_target_path, mat), list_targets), total=len(list_targets)))\n",
    "\n",
    "#### without multithreading\n",
    "# for data_path in tqdm(list_datas, total=len(list_datas)):\n",
    "#     process_file(data_path, dataset_data_path, mat)\n",
    "\n",
    "# for target_path in tqdm(list_targets, total=len(list_targets)):\n",
    "#     process_file(target_path, dataset_target_path, mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\ you will need to edit this with regards to the number of modalities used;\n",
    "data_dataset_json = {\n",
    "    \"labels\": {\n",
    "        \"label_001\": \"1\", \n",
    "        \"background\": 0\n",
    "    },\n",
    "    \"channel_names\": {\n",
    "        \"0\": \"MR\",\n",
    "    },\n",
    "    \"numTraining\": len(list_datas),\n",
    "    \"file_ending\": \".nii.gz\"\n",
    "}\n",
    "dump_data_datasets_path = os.path.join(dataset_data_path, 'dataset.json')\n",
    "with open(dump_data_datasets_path, 'w') as f:\n",
    "    json.dump(data_dataset_json, f)\n",
    "\n",
    "target_dataset_json = {\n",
    "    \"labels\": {\n",
    "        \"label_001\": \"1\",\n",
    "        \"background\": 0\n",
    "    },\n",
    "    \"channel_names\": {\n",
    "        \"0\": \"CT\",\n",
    "    },\n",
    "    \"numTraining\": len(list_targets),\n",
    "    \"file_ending\": \".nii.gz\"\n",
    "}\n",
    "dump_target_datasets_path = os.path.join(dataset_target_path, 'dataset.json')\n",
    "with open(dump_target_datasets_path, 'w') as f:\n",
    "    json.dump(target_dataset_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply preprocessing and unpacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'MPLBACKEND' in os.environ: \n",
    "    del os.environ['MPLBACKEND'] # avoid conflicts with matplotlib backend  \n",
    "    \n",
    "os.system(f'nnUNetv2_plan_and_preprocess -d {dataset_id} -c 3d_fullres')\n",
    "os.system(f'nnUNetv2_unpack {dataset_id} 3d_fullres 0')\n",
    "\n",
    "os.system(f'nnUNetv2_plan_and_preprocess -d {dataset_id + 1} -c 3d_fullres')\n",
    "os.system(f'nnUNetv2_unpack {dataset_id + 1} 3d_fullres 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define 2nd modality raw data as gt_segmentations of 1st modality\n",
    "##### originally used for computing metrics / postprocessing, not sure if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed/Dataset050_SynthRAD2023_Pelvis_MR\n",
      "raw/Dataset051_SynthRAD2023_Pelvis_CT/imagesTr/1PA070_0000.nii.gz -> preprocessed/Dataset050_SynthRAD2023_Pelvis_MR/gt_segmentations/1PA070.nii.gz\n",
      "raw/Dataset051_SynthRAD2023_Pelvis_CT/imagesTr/1PA073_0000.nii.gz -> preprocessed/Dataset050_SynthRAD2023_Pelvis_MR/gt_segmentations/1PA073.nii.gz\n",
      "raw/Dataset051_SynthRAD2023_Pelvis_CT/imagesTr/1PA074_0000.nii.gz -> preprocessed/Dataset050_SynthRAD2023_Pelvis_MR/gt_segmentations/1PA074.nii.gz\n",
      "raw/Dataset051_SynthRAD2023_Pelvis_CT/imagesTr/1PA076_0000.nii.gz -> preprocessed/Dataset050_SynthRAD2023_Pelvis_MR/gt_segmentations/1PA076.nii.gz\n",
      "raw/Dataset051_SynthRAD2023_Pelvis_CT/imagesTr/1PA079_0000.nii.gz -> preprocessed/Dataset050_SynthRAD2023_Pelvis_MR/gt_segmentations/1PA079.nii.gz\n"
     ]
    }
   ],
   "source": [
    "nnunet_datas_preprocessed_dir = os.path.join(os.environ['nnUNet_preprocessed'], f'Dataset{dataset_id+1:03d}_{dataset_target_name}') \n",
    "nnunet_targets_preprocessed_dir = os.path.join(os.environ['nnUNet_preprocessed'], f'Dataset{dataset_id:03d}_{dataset_data_name}') \n",
    "\n",
    "list_targets = glob.glob(os.path.join(f\"{dataset_target_path}/imagesTr\", '*'))\n",
    "list_targets.sort()\n",
    "list_gt_segmentations_datas = glob.glob(os.path.join(f\"{nnunet_targets_preprocessed_dir}/gt_segmentations\", '*'))\n",
    "list_gt_segmentations_datas.sort()\n",
    "\n",
    "print(nnunet_targets_preprocessed_dir)\n",
    "\n",
    "for (preprocessed_path, gt_path) in zip(list_targets, list_gt_segmentations_datas):\n",
    "    # here, gt_path is the path to the gt_segmentation in nnUNet_preprocessed.\n",
    "    print(preprocessed_path, \"->\", gt_path) # ensure correct file pairing; \n",
    "    shutil.copy(src = preprocessed_path, dst = gt_path) # we use shutil.copy to ensure safety, but switching to shutil.move would be more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define 2nd modality preprocessed files as ground truth of 1st modality\n",
    "##### used in training, definitely needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed/Dataset051_SynthRAD2023_Pelvis_CT/nnUNetPlans_3d_fullres/1PA070.npy -> preprocessed/Dataset050_SynthRAD2023_Pelvis_MR/nnUNetPlans_3d_fullres/1PA070_seg.npy\n",
      "preprocessed/Dataset051_SynthRAD2023_Pelvis_CT/nnUNetPlans_3d_fullres/1PA073.npy -> preprocessed/Dataset050_SynthRAD2023_Pelvis_MR/nnUNetPlans_3d_fullres/1PA073_seg.npy\n",
      "preprocessed/Dataset051_SynthRAD2023_Pelvis_CT/nnUNetPlans_3d_fullres/1PA074.npy -> preprocessed/Dataset050_SynthRAD2023_Pelvis_MR/nnUNetPlans_3d_fullres/1PA074_seg.npy\n",
      "preprocessed/Dataset051_SynthRAD2023_Pelvis_CT/nnUNetPlans_3d_fullres/1PA076.npy -> preprocessed/Dataset050_SynthRAD2023_Pelvis_MR/nnUNetPlans_3d_fullres/1PA076_seg.npy\n",
      "preprocessed/Dataset051_SynthRAD2023_Pelvis_CT/nnUNetPlans_3d_fullres/1PA079.npy -> preprocessed/Dataset050_SynthRAD2023_Pelvis_MR/nnUNetPlans_3d_fullres/1PA079_seg.npy\n"
     ]
    }
   ],
   "source": [
    "list_preprocessed_datas_seg_path = sorted(glob.glob(os.path.join(nnunet_targets_preprocessed_dir, 'nnUNetPlans_3d_fullres/*_seg.npy')))\n",
    "\n",
    "list_preprocessed_targets_path = sorted(glob.glob(os.path.join(nnunet_datas_preprocessed_dir, 'nnUNetPlans_3d_fullres/*.npy')))\n",
    "list_preprocessed_targets_path = [name for name in list_preprocessed_targets_path if '_seg' not in name]\n",
    "\n",
    "for (datas_path, targets_path) in zip(list_preprocessed_datas_seg_path, list_preprocessed_targets_path):\n",
    "    print(targets_path, \"->\", datas_path)\n",
    "    shutil.copy(src = targets_path, dst = datas_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's it!\n",
    "You should be able to start training with : \n",
    "```\n",
    "export nnUNet_raw=\"/data/alonguefosse/nnUNet/raw\"\n",
    "export nnUNet_preprocessed=\"/data/alonguefosse/nnUNet/preprocessed\"\n",
    "export nnUNet_results=\"/data/alonguefosse/nnUNet/results\"\n",
    "\n",
    "nnUNetv2_train 50 3d_fullres 0 -tr nnUNetTrainerMRCT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
